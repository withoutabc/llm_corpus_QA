# 医生知识库

该功能目前没有完整实现，因为暂时缺少openai api key的资源，预计在后期实现，下面只展示设计思路，以及必要代码。

## 功能描述

医生需要大量的知识储备，这需要他们阅读很多专业书籍，而在遇到实际问题时往往很难快速从书籍中找到答案，网络上的答案又缺乏一定专业性。因此，医生知识库的功能允许医生在平台存储书籍等资料（暂时只接受pdf和markdown），运用AIGC大模型智能化学习知识并解答医生的问题，从而起到辅助诊疗的作用。

此外，LLM对部分专业问题的回答并不是很好，而加上本地知识，就可以帮助 LLM 做出更好的回答。另外，也有助于缓解大模型的“幻觉”问题。

## 向量数据库搭建

导包

```python
# 首先实现基本配置
from langchain.vectorstores import Chroma  # 导入Chroma向量存储库
from langchain.document_loaders import PyMuPDFLoader  # 导入PyMuPDFLoader文档加载库
from langchain.text_splitter import RecursiveCharacterTextSplitter  # 导入RecursiveCharacterTextSplitter文本拆分库
from langchain.document_loaders import UnstructuredMarkdownLoader  # 导入UnstructuredMarkdownLoader文档加载库
from langchain.document_loaders import UnstructuredFileLoader  # 导入UnstructuredFileLoader文档加载库

from langchain.embeddings.openai import OpenAIEmbeddings  # 导入OpenAIEmbeddings嵌入库
from langchain.embeddings.huggingface import HuggingFaceEmbeddings  # 导入HuggingFaceEmbeddings嵌入库

from langchain.llms import OpenAI  # 导入OpenAI LLMS（Language Model Microservice）库
from langchain.llms import HuggingFacePipeline  # 导入HuggingFacePipeline库

# 使用前配置自己的 api 到环境变量中
import os  # 导入os库
import openai  # 导入openai库
import sys  # 导入sys库

from dotenv import load_dotenv, find_dotenv  # 导入dotenv库

_ = load_dotenv(find_dotenv())  # 加载环境变量文件
openai.api_key  = os.environ['OPENAI_API_KEY']  # 设置OpenAI的API密钥
```

导入资源（pdf和md）

> 读取方式可以用文件流替代

```python
# 指定 PDF 文件所在的文件夹路径
folder_path = "../../data_base/knowledge_db/prompt_engineering/"

# 获取文件夹中的所有文件名
files = os.listdir(folder_path)

# 创建加载器列表
loaders = []

# 遍历文件列表
for one_file in files:
    # 根据文件路径创建 PyMuPDFLoader 加载器
    loader = PyMuPDFLoader(os.path.join(folder_path, one_file))
    
    # 将加载器添加到列表中
    loaders.append(loader)

# 创建文档列表
docs = []

# 遍历加载器列表
for loader in loaders:
    # 加载文档并将其添加到文档列表中
    docs.extend(loader.load())
```

```python
# 指定 Markdown 文件所在的文件夹路径
folder_path = "../../data_base/knowledge_db/prompt_engineering/"

# 获取文件夹中的所有文件名
files = os.listdir(folder_path)

# 创建加载器列表
loaders = []

# 遍历文件列表
for one_file in files:
    # 根据文件路径创建 UnstructuredMarkdownLoader 加载器
    loader = UnstructuredMarkdownLoader(os.path.join(folder_path, one_file))
    
    # 将加载器添加到列表中
    loaders.append(loader)

# 创建文档列表
docs = []

# 遍历加载器列表
for loader in loaders:
    # 加载文档并将其添加到文档列表中
    docs.extend(loader.load())
```

对加载好的文档进行切片并向量化后存储到向量数据库

```python
# 切分文档
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=150)
split_docs = text_splitter.split_documents(docs)


# 定义 Embeddings
embedding = OpenAIEmbeddings() 

# 定义持久化路径
persist_directory = '../../data_base/vector_db/chroma'

# 加载数据库
vectordb = Chroma.from_documents(
    documents=split_docs,
    embedding=embedding,
    persist_directory=persist_directory  # 允许我们将persist_directory目录保存到磁盘上
)

# 向量数据库持久化
vectordb.persist()
```

## 问答助手——Prompt设计

加载向量数据库

```python
from langchain.vectorstores import Chroma
from langchain.embeddings.openai import OpenAIEmbeddings    # 调用 OpenAI 的 Embeddings 模型
import openai
from dotenv import load_dotenv, find_dotenv
import os

_ = load_dotenv(find_dotenv()) # 读取本地的 .env 文件
openai.api_key = os.environ['OPENAI_API_KEY']

# 定义 Embeddings
embedding = OpenAIEmbeddings() 

# 向量数据库持久化路径
persist_directory = '../../data_base/vector_db/chroma'

# 加载数据库
vectordb = Chroma(
    persist_directory=persist_directory,  # 允许我们将 persist_directory 目录保存到磁盘上
    embedding_function=embedding
)
```

创建LLM

```python
from langchain.chat_models import ChatOpenAI

# 创建 ChatOpenAI 实例
llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)

# 使用 ChatOpenAI 实例进行预测
llm.predict("你好")
```

构建prompt

```python
from langchain.prompts import PromptTemplate

template = """使用以下上下文来回答最后的问题。如果你不知道答案，就说你不知道，不要试图编造答
案。最多使用三句话。尽量使答案简明扼要。总是在回答的最后说“谢谢你的提问！”。
{context}
问题: {question}
有用的回答:"""

# 创建 PromptTemplate 实例
QA_CHAIN_PROMPT = PromptTemplate(input_variables=["context","question"],
                                 template=template)

from langchain.chains import RetrievalQA

# 创建一个基于模板的检索链：
qa_chain = RetrievalQA.from_chain_type(llm,
                                       retriever=vectordb.as_retriever(),
                                       return_source_documents=True,
                                       chain_type_kwargs={"prompt":QA_CHAIN_PROMPT})
```

效果测试

```python
quesion = "..."
result = qa_chain({"query": question})
print("大模型+知识库后回答 question 的结果：")
print(result["result"])
```

## 添加历史对话的记忆功能

使用 `ConversationBufferMemory` ，它保存聊天消息历史记录的列表，这些历史记录将在回答问题时与问题一起传递给聊天机器人，从而将它们添加到上下文中。

```python
from langchain.memory import ConversationBufferMemory

memory = ConversationBufferMemory(
    memory_key="chat_history",  # 与 prompt 的输入变量保持一致。
    return_messages=True  # 将以消息列表的形式返回聊天记录，而不是单个字符串
)
```

对话检索链

```python
# 在检索 QA 链的基础上，增加了处理对话历史的能力。

# 先加载向量数据库和llm
from langchain.vectorstores import Chroma
from langchain.embeddings.openai import OpenAIEmbeddings
import openai
from dotenv import load_dotenv, find_dotenv
import os

_ = load_dotenv(find_dotenv()) 
openai.api_key = os.environ['OPENAI_API_KEY']

# 定义 Embeddings
embedding = OpenAIEmbeddings() 
# 向量数据库持久化路径
persist_directory = '../../data_base/vector_db/chroma'
# 加载数据库
vectordb = Chroma(
    persist_directory=persist_directory,  # 允许我们将persist_directory目录保存到磁盘上
    embedding_function=embedding
)

# 创建LLM
from langchain.chat_models import ChatOpenAI
llm = ChatOpenAI(model_name = "gpt-3.5-turbo", temperature = 0 )
```

提出问题

```python
# 提出第一个问题，此时无历史对话
from langchain.chains import ConversationalRetrievalChain

retriever=vectordb.as_retriever()

qa = ConversationalRetrievalChain.from_llm(
    llm,
    retriever=retriever,
    memory=memory
)
question = "第一个问题"
result = qa({"question": question})
print(result['answer'])

# 提出第二个问题，以上一个问题作为上下文
question = "第二个问题"
result = qa({"question": question})
print(result['answer'])
```

